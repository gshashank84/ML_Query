Why the cost function is computed for all the training sets whereas the square loss function is computed for each training sets?
shape of w?

Why TanH function is used over to sigmoid function as an activation function ?
-- The tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer.

Why we initialize weights randomly rather than initializing it to zero?
-- Logistic regression’s weights w should be initialized randomly rather than to all zeros, because if you initialize to all zeros, then logistic regression will fail to learn a useful decision boundary because it will fail to “break symmetry”.

The shape of a Weights of any layer L neural network can be determined by-
(no of units in the layer L, no of units in layer L-1)

Why L2 Regularization is termed as decay rate?
How does Regularization helps from overfitting?
-- As we add the extra term lamba times the weights euclidean by no of training examples in the cost function and also for dw in backprop.
Since the values initialized of weight matrix is containing less than 1, then the gradient descent becomes slow because of the small
values updated. 

What is Mini-batch Gradient Descent?
-- Rather than taking all the training examples one at a time to compute loss (Batch Gradient Descent) we actually divide the training
examples into a set of mini-batches. These set of mini-batches are used to compute loss and backprop and update the parameters, which
will be used for the gradient descent of another set of training examples.
tip- we usually set 2^n training examples in a set consisting the total training examples.

What is Gradient Descent with Momentum?
-- When we do Gradient Descent it turns out to be oscillating up and down with moving forward to the global minimum. This oscillating
nature makes it slow, so to avoid the oscillating nature of this we take averages to make it smoother. What actually happens is that when
we take average in the horizontal direction then the up and down distances cancel out to give a less oscillating nature. The average
is calcluated by the hyperparameter Beta(usually 0.9). The formaula : Vt = Beta*(Vt-1)+(1-Beta)*(Theta) where theta represents the 
current value whereas the Vt-1 represents the average of the values of the past datapoints.
The averages we took is usually termed to be exponentially weighted averages.

What is Adam?
combination of gradient descent with momentum and RMSProp.