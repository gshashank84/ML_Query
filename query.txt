Why the cost function is computed for all the training sets whereas the square loss function is computed for each training sets?
shape of w?

Why TanH function is used over to sigmoid function as an activation function ?
-- The tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer.

Why we initialize weights randomly rather than initializing it to zero?
-- Logistic regression’s weights w should be initialized randomly rather than to all zeros, because if you initialize to all zeros, then logistic regression will fail to learn a useful decision boundary because it will fail to “break symmetry”.

The shape of a Weights of any layer L neural network can be determined by-
(no of units in the layer L, no of units in layer L-1)

Why L2 Regularization is termed as decay rate?
How does Regularization helps from overfitting?
-- As we add the extra term lamba times the weights euclidean by no of training examples in the cost function and also for dw in backprop.
Since the values initialized of weight matrix is containing less than 1, then the gradient descent becomes slow because of the small
values updated. 

What is Mini-batch Gradient Descent?
-- Rather than taking all the training examples one at a time to compute loss (Batch Gradient Descent) we actually divide the training
examples into a set of mini-batches. These set of mini-batches are used to compute loss and backprop and update the parameters, which
will be used for the gradient descent of another set of training examples.
tip- we usually set 2^n training examples in a set consisting the total training examples.